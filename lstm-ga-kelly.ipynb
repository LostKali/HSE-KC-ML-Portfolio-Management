{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3436192b-0051-4fb7-9150-0349b7a29fc2",
   "metadata": {},
   "source": [
    "# Этап 1: Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b157e-4e9b-4b5f-b196-5cbbe644c44a",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e458641a-a5dd-4d10-9825-d63e772bd97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Импорт стандартных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# Для разделения данных и предобработки\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Для построения модели LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "# Для генетического алгоритма\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Для оценки модели\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Для визуализации\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Дополнительные библиотеки\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "32f8fcbe-2a3e-47bf-a3c8-5f84a3f17357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/sp500_historical_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02a6e8-530b-4409-9c13-0d845cf73e49",
   "metadata": {},
   "source": [
    "#### Преобразование столбца даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "67ad233c-0ba6-4815-848f-d9549f10ed95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e8fd4-2d86-4d35-b239-cd860eae2f74",
   "metadata": {},
   "source": [
    "#### Удаление данных за выходные дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f53168d0-609a-47c8-881d-de31e48280f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[data['Date'].dt.weekday < 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbf4b5-3e29-483c-9eea-4880e451bd0c",
   "metadata": {},
   "source": [
    "#### Определение полного диапазона дат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1b42412c-1295-4673-a6b1-523651cf127c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_date_range = pd.date_range(start=data['Date'].min(), end=data['Date'].max(), freq='B')  # 'B' обозначает бизнес-день"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08be41-6ecd-46fd-b9c2-410ea99d82ea",
   "metadata": {},
   "source": [
    "#### Проверка наличия данных по каждому тикеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bc23a5c0-1335-43a9-84c9-7e323c77d1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pivot = data.pivot_table(index='Date', columns='Ticker', values='Adj Close')\n",
    "missing_percent = data_pivot.isnull().mean() * 100\n",
    "tickers_to_remove = missing_percent[missing_percent > 5].index.tolist()\n",
    "clean_data = data[~data['Ticker'].isin(tickers_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129b7cb-ed8e-413a-95f3-4a3365b2cb96",
   "metadata": {},
   "source": [
    "#### Проверка на дупликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31568333-bc8c-4c79-ae39-9a3e7459a859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_data = clean_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d627aac-121f-4348-906c-d3ad3c70d7e4",
   "metadata": {},
   "source": [
    "#### Обработка подозрительно низкого объема торгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6e62f7e3-6f03-44b8-acb4-f3dcf9793178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "volume_threshold = clean_data['Volume'].quantile(0.01)\n",
    "low_volume_data = clean_data[clean_data['Volume'] <= volume_threshold]\n",
    "low_volume_data.to_csv('data/low_volume_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276abb6-b1e8-49a4-824f-62f774c426da",
   "metadata": {},
   "source": [
    "#### Проверка размерности данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "658c43d8-f800-4a27-ac69-a0b4012a7a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pivot_clean = clean_data.pivot_table(index='Date', columns='Ticker', values='Adj Close')\n",
    "missing_dates_per_ticker = data_pivot_clean.isnull().sum()\n",
    "tickers_with_missing_dates = missing_dates_per_ticker[missing_dates_per_ticker > 0].index.tolist()\n",
    "clean_data = clean_data[~clean_data['Ticker'].isin(tickers_with_missing_dates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc594fb4-b964-4686-aecd-ddaf647c8e19",
   "metadata": {},
   "source": [
    "#### Сохранение очищенных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9fc75e58-8b78-40a8-abc7-0c2442c9f43d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_tickers = clean_data['Ticker'].unique()\n",
    "cleaned_data = data.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "clean_data.to_csv('data/cleaned_sp500_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67968b9-44b5-4cac-95cc-df1c8612aede",
   "metadata": {},
   "source": [
    "#### Разделение данных на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ea6cecf1-1e7a-4583-9747-a9d00ea21490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дата разделения данных: 2022-05-25\n",
      "Размер обучающей выборки: 298427 записей\n",
      "Размер тестовой выборки: 74756 записей\n"
     ]
    }
   ],
   "source": [
    "# Получение минимальной и максимальной даты\n",
    "min_date = data['Date'].min()\n",
    "max_date = data['Date'].max()\n",
    "\n",
    "# Общий период времени\n",
    "total_days = (max_date - min_date).days\n",
    "\n",
    "# Дата разделения (80% от общего периода)\n",
    "split_date = min_date + pd.Timedelta(days=int(total_days * 0.8))\n",
    "\n",
    "print(f\"Дата разделения данных: {split_date.date()}\")\n",
    "# Обучающая выборка: данные до даты разделения (включительно)\n",
    "train_data = data[data['Date'] <= split_date]\n",
    "\n",
    "# Тестовая выборка: данные после даты разделения\n",
    "test_data = data[data['Date'] > split_date]\n",
    "print(f\"Размер обучающей выборки: {len(train_data)} записей\")\n",
    "print(f\"Размер тестовой выборки: {len(test_data)} записей\")\n",
    "\n",
    "\n",
    "# Сохранение обучающей выборки\n",
    "train_data.to_csv('data/sp500_train_data.csv', index=False)\n",
    "\n",
    "# Сохранение тестовой выборки\n",
    "test_data.to_csv('data/sp500_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf16ce0-743c-4585-9841-bacc1c75990c",
   "metadata": {},
   "source": [
    "#### Сбор информации о секторах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dc875be1-2972-450d-b7b8-4f496e1d0a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Получение списка уникальных тикеров\n",
    "tickers = data['Ticker'].unique()\n",
    "\n",
    "# Инициализация словаря для хранения информации о секторах\n",
    "ticker_sectors = {}\n",
    "\n",
    "# Получение информации о секторах для каждого тикера\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock_info = yf.Ticker(ticker).info\n",
    "        sector = stock_info.get('sector', 'Unknown')\n",
    "        ticker_sectors[ticker] = sector\n",
    "    except Exception as e:\n",
    "        print(f\"Не удалось получить данные для {ticker}: {e}\")\n",
    "        ticker_sectors[ticker] = 'Unknown'\n",
    "\n",
    "# Создание DataFrame из словаря\n",
    "sectors_df = pd.DataFrame.from_dict(ticker_sectors, orient='index', columns=['Sector'])\n",
    "\n",
    "sectors_df.to_csv('data/sectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9295617-4cb2-45ac-a92f-b2c0bde2a5c5",
   "metadata": {},
   "source": [
    "#### Сохранение волатильности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d446ed75-169b-4df0-9f52-83bef5c3cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    # Получение данных для тикера\n",
    "    df_ticker = data[data['Ticker'] == ticker].sort_values('Date')\n",
    "\n",
    "    # Используем 'Adj Close' в качестве целевой переменной\n",
    "    close_prices = df_ticker['Adj Close'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Расчет доходностей для оценки волатильности\n",
    "    returns = np.log(close_prices[1:] / close_prices[:-1])\n",
    "\n",
    "    # Оценка волатильности (стандартное отклонение доходностей)\n",
    "    volatility_value = np.std(returns)\n",
    "    volatility[ticker] = volatility_value\n",
    "    \n",
    "# Преобразование словаря волатильности в DataFrame\n",
    "volatility_df = pd.DataFrame.from_dict(volatility, orient='index', columns=['Volatility'])\n",
    "volatility_df.to_csv('data/volatility.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cda131-ab1b-4486-a65f-128a75f67ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Этап 2: Построение и оптимизация модели LSTM с помощью GA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1014f0-1b08-47cc-968f-e7058f4fe227",
   "metadata": {},
   "source": [
    "#### Загрузка очищенных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2a62bdda-ce2c-4733-b073-c0a5e8edcbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загрузка обучающих данных\n",
    "train_data = pd.read_csv('data/sp500_train_data.csv')\n",
    "\n",
    "# Преобразование столбца 'Date' в формат datetime\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "\n",
    "# Сортировка данных\n",
    "train_data = train_data.sort_values(['Ticker', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1376d-00d1-437b-a132-2c0dd7d60969",
   "metadata": {},
   "source": [
    "#### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0991312c-39ad-4cf3-9914-c1f9173616fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определение параметров\n",
    "TIME_STEPS = 30  # Длина последовательности для входа в LSTM\n",
    "\n",
    "# Инициализация словарей для хранения результатов\n",
    "models = {}\n",
    "scalers = {}\n",
    "volatility = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fac95-f43f-4ad0-851f-442fba6ca898",
   "metadata": {},
   "source": [
    "#### Установка random для воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de7e79ab-2508-412b-9934-c49541e412d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6bfc3c-f221-44a1-b83d-d787b51ab2f7",
   "metadata": {},
   "source": [
    "#### Определение функций для подготовки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d7426f2-5de6-43aa-ae12-328fa4abf6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, time_steps=TIME_STEPS):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:(i + time_steps)])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebab5c-0b16-4008-b54b-0ec1ab0eb22e",
   "metadata": {},
   "source": [
    "#### Определение функции оценки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e6192f2-2dd4-4caa-98ff-27909e97d21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def evaluate_model(individual, X_train, y_train, X_val, y_val):\n",
    "    # Unpack hyperparameters\n",
    "    n_layers = individual[0]\n",
    "    n_neurons = individual[1]\n",
    "    dropout_rate = individual[2]\n",
    "    learning_rate = individual[3]\n",
    "\n",
    "    # Build the model\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "    if n_layers == 1:\n",
    "        # Single LSTM layer\n",
    "        model.add(LSTM(units=n_neurons, return_sequences=False))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    else:\n",
    "        # First LSTM layer\n",
    "        model.add(LSTM(units=n_neurons, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        # Middle LSTM layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            model.add(LSTM(units=n_neurons, return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        # Last LSTM layer\n",
    "        model.add(LSTM(units=n_neurons, return_sequences=False))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=32,\n",
    "                        validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "    # Evaluate validation loss\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_loss,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a477a53-f2be-48d5-9289-67d48c83eb32",
   "metadata": {},
   "source": [
    "#### Определение процесса генетического алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1967954-53bf-41f1-a992-786c6f00f418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_mutation(individual, indpb):\n",
    "    for i in range(len(individual)):\n",
    "        if random.random() < indpb:\n",
    "            if i == 0:  # n_layers (целое число)\n",
    "                individual[i] = random.randint(1, 2)  # Максимум 2 слоя\n",
    "            elif i == 1:  # n_neurons (целое число)\n",
    "                individual[i] = random.randint(32, 128)  # От 32 до 128 нейронов\n",
    "            elif i == 2:  # dropout_rate (вещественное число)\n",
    "                individual[i] = random.uniform(0.0, 0.3)  # От 0.0 до 0.3\n",
    "            elif i == 3:  # learning_rate (вещественное число)\n",
    "                individual[i] = random.uniform(0.0005, 0.005)  # Уменьшенный диапазон\n",
    "    return individual,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6811865f-f93a-4f6f-b3a4-69ff17be345f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_model_with_ga(X_train, y_train, X_val, y_val, n_generations=3, population_size=5):\n",
    "    # Определение индивидуала и функции приспособленности\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Минимизируем валидационную ошибку\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # Определение атрибутов для каждого гиперпараметра\n",
    "    toolbox.register(\"attr_n_layers\", random.randint, 1, 2)\n",
    "    toolbox.register(\"attr_n_neurons\", random.randint, 32, 128)\n",
    "    toolbox.register(\"attr_dropout_rate\", random.uniform, 0.0, 0.3)\n",
    "    toolbox.register(\"attr_learning_rate\", random.uniform, 0.0005, 0.005)\n",
    "\n",
    "    # Создание индивидуала\n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                     (toolbox.attr_n_layers, toolbox.attr_n_neurons, toolbox.attr_dropout_rate, toolbox.attr_learning_rate), n=1)\n",
    "\n",
    "    # Создание популяции\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    # Определение функции оценки\n",
    "    def eval_function(individual):\n",
    "        return evaluate_model(individual, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    toolbox.register(\"evaluate\", eval_function)\n",
    "    toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "    toolbox.register(\"mutate\", custom_mutation, indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # Инициализация популяции\n",
    "    pop = toolbox.population(n=population_size)\n",
    "\n",
    "    # Использование Hall of Fame для отслеживания лучшего решения\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    # Параллельная оценка популяции\n",
    "    def eval_population(population):\n",
    "        fitnesses = Parallel(n_jobs=-1)(delayed(toolbox.evaluate)(ind) for ind in population)\n",
    "        for ind, fit in zip(population, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    # Основной цикл GA\n",
    "    for gen in range(n_generations):\n",
    "        print(f\"Поколение {gen+1}/{n_generations}\")\n",
    "\n",
    "        # Оценка популяции\n",
    "        invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "        if invalid_ind:\n",
    "            eval_population(invalid_ind)\n",
    "\n",
    "        # Обновление Hall of Fame\n",
    "        hof.update(pop)\n",
    "\n",
    "        # Применение операторов GA\n",
    "        offspring = algorithms.varAnd(pop, toolbox, cxpb=0.5, mutpb=0.2)\n",
    "\n",
    "        # Оценка потомков\n",
    "        invalid_offspring = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        if invalid_offspring:\n",
    "            eval_population(invalid_offspring)\n",
    "        \n",
    "        # Вывод лучшей валидационной ошибки в текущем поколении\n",
    "        best = tools.selBest(pop, k=1)[0]\n",
    "        print(f\"Лучшая валидационная ошибка: {best.fitness.values[0]}\")\n",
    "\n",
    "        # Выбор новой популяции\n",
    "        pop = toolbox.select(pop + offspring, k=len(pop))\n",
    "\n",
    "    # Возвращаем лучший найденный индивидуал\n",
    "    best_individual = hof[0]\n",
    "    return best_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a2089-6ea2-4f64-bbe8-1460e4a2a766",
   "metadata": {},
   "source": [
    "#### Обучение модели для каждого тикера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8d4a5-8b97-4195-b7d0-7b3d19287dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создание каталогов для сохранения моделей и графиков\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "tickers = train_data['Ticker'].unique()\n",
    "\n",
    "# Цикл по каждому тикеру\n",
    "for ticker in tickers:\n",
    "    if not os.path.isfile(f'models/{ticker}_model.keras'):\n",
    "        print(f\"Обработка тикера: {ticker}\")\n",
    "\n",
    "        # Получение данных для тикера\n",
    "        df_ticker = train_data[train_data['Ticker'] == ticker].sort_values('Date')\n",
    "\n",
    "        # Используем 'Adj Close' в качестве целевой переменной\n",
    "        close_prices = df_ticker['Adj Close'].values.reshape(-1, 1)\n",
    "\n",
    "        # Проверка достаточности данных\n",
    "        if len(close_prices) <= TIME_STEPS:\n",
    "            print(f\"Недостаточно данных для тикера {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # Масштабирование данных\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(close_prices)\n",
    "        scalers[ticker] = scaler\n",
    "        \n",
    "        # Сохранение скейлера\n",
    "        scaler_filename = f'scalers/{ticker}_scaler.save'\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "        # Создание последовательностей\n",
    "        X, y = create_sequences(scaled_data)\n",
    "\n",
    "        # Проверка достаточности последовательностей\n",
    "        if len(X) == 0:\n",
    "            print(f\"Недостаточно последовательностей для тикера {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # Разделение данных на обучающую и тестовую выборки (80% на обучение)\n",
    "        split_index = int(0.8 * len(X))\n",
    "        X_train_full, X_test = X[:split_index], X[split_index:]\n",
    "        y_train_full, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "        # Дополнительное разделение обучающих данных на обучение и валидацию (80% на обучение)\n",
    "        val_split_index = int(0.8 * len(X_train_full))\n",
    "        X_train, X_val = X_train_full[:val_split_index], X_train_full[val_split_index:]\n",
    "        y_train, y_val = y_train_full[:val_split_index], y_train_full[val_split_index:]\n",
    "\n",
    "        # Оптимизация модели с помощью GA\n",
    "        best_hyperparams = optimize_model_with_ga(X_train, y_train, X_val, y_val, n_generations=3, population_size=5)\n",
    "\n",
    "        # Извлечение лучших гиперпараметров\n",
    "        n_layers = best_hyperparams[0]\n",
    "        n_neurons = best_hyperparams[1]\n",
    "        dropout_rate = best_hyperparams[2]\n",
    "        learning_rate = best_hyperparams[3]\n",
    "\n",
    "        # Построение финальной модели с лучшими гиперпараметрами\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train_full.shape[1], X_train_full.shape[2])))\n",
    "\n",
    "        if n_layers == 1:\n",
    "            # Single LSTM layer\n",
    "            model.add(LSTM(units=n_neurons, return_sequences=False))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        else:\n",
    "            # First LSTM layer\n",
    "            model.add(LSTM(units=n_neurons, return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            # Middle LSTM layers\n",
    "            for _ in range(n_layers - 2):\n",
    "                model.add(LSTM(units=n_neurons, return_sequences=True))\n",
    "                model.add(Dropout(dropout_rate))\n",
    "            # Last LSTM layer\n",
    "            model.add(LSTM(units=n_neurons, return_sequences=False))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "        # Обучение финальной модели на полной обучающей выборке\n",
    "        model.fit(X_train_full, y_train_full, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "        # Оценка модели на тестовых данных\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_inverse = scaler.inverse_transform(y_pred)\n",
    "        y_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Расчет метрик оценки\n",
    "        mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "        mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "        performance[ticker] = {'MSE': mse, 'MAE': mae}\n",
    "\n",
    "        # Сохранение модели\n",
    "        model.save(f'models/{ticker}_model.keras')\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Визуализация результатов\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_test_inverse, label='Реальная цена')\n",
    "        plt.plot(y_pred_inverse, label='Предсказанная цена')\n",
    "        plt.title(f'Предсказание цены для {ticker}')\n",
    "        plt.xlabel('Время')\n",
    "        plt.ylabel('Цена')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'plots/{ticker}_prediction.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"Модель для тикера: {ticker} уже создана\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dd8bf-9ffc-43c1-a4e0-e94c2d415792",
   "metadata": {},
   "source": [
    "#### Сохранение метрик модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883296e-38bc-4f8a-85e6-28b106bef6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование словаря производительности в DataFrame\n",
    "performance_df = pd.DataFrame.from_dict(performance, orient='index')\n",
    "performance_df.to_csv('data/model_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547991a-b482-4a81-a244-1582739b76b8",
   "metadata": {},
   "source": [
    "# Этап 3: Формирование портфеля"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a662926-8650-4d5f-9890-60ed9d5c0850",
   "metadata": {},
   "source": [
    "#### Загрузка сохраненных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5d1e7d2b-29f9-4603-a60f-554c847b99b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загрузка тестовых данных\n",
    "test_data = pd.read_csv('data/sp500_test_data.csv')\n",
    "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
    "test_data = test_data.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Определяем уникальные даты\n",
    "unique_dates = test_data['Date'].unique()\n",
    "total_dates = len(unique_dates)\n",
    "train_size = int(total_dates * 0.8)\n",
    "train_dates = unique_dates[:train_size]\n",
    "test_dates = unique_dates[train_size:]\n",
    "\n",
    "# Получение списка чистых тикеров\n",
    "clean_tickers = test_data['Ticker'].unique()\n",
    "\n",
    "# Загрузка волатильности из файла\n",
    "volatility_df = pd.read_csv('data/volatility.csv', index_col=0)\n",
    "volatility = volatility_df['Volatility'].to_dict()\n",
    "\n",
    "# Загрузка информации о секторах\n",
    "sectors_df = pd.read_csv('data/sectors.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade85a40-c9d9-4f92-aadd-d75a9283c21f",
   "metadata": {},
   "source": [
    "#### Загрузка сохраненных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ad694-2240-4ca2-82b0-9f68f14d3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Загрузка сохраненных моделей\n",
    "models = {}\n",
    "scalers = {}\n",
    "for ticker in clean_tickers:\n",
    "    scaler_filename = f'scalers/{ticker}_scaler.save'\n",
    "    if os.path.exists(scaler_filename):\n",
    "        scalers[ticker] = joblib.load(scaler_filename)\n",
    "    else:\n",
    "        print(f\"Скейлер для тикера {ticker} не найден.\")\n",
    "        continue\n",
    "    model_path = f'models/{ticker}_model.keras'\n",
    "    if os.path.exists(model_path):\n",
    "        models[ticker] = load_model(model_path)\n",
    "    else:\n",
    "        print(f\"Модель для тикера {ticker} не найдена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4350af5-154a-4d9f-9a92-c29f02108139",
   "metadata": {},
   "source": [
    "#### Подготовка данных для предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "345f8fc5-912d-484a-b145-914a6235565d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIME_STEPS = 30  # Длина последовательности для входа в LSTM\n",
    "\n",
    "# Дата, на которую мы делаем предсказание (последняя доступная дата)\n",
    "prediction_date = data['Date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e541d-35d3-47a0-b159-b3748c1c9221",
   "metadata": {},
   "source": [
    "#### Составление портфеля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76945b-98a7-43ab-90aa-d154d475d2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Словарь для хранения ожидаемой доходности и волатильности\n",
    "expected_returns = {}\n",
    "\n",
    "for ticker in clean_tickers:\n",
    "    if ticker not in models:\n",
    "        continue\n",
    "    # Получаем данные для тикера\n",
    "    df_ticker = data[data['Ticker'] == ticker].sort_values('Date')\n",
    "    \n",
    "    # Проверяем, есть ли достаточное количество данных\n",
    "    if len(df_ticker) < TIME_STEPS + 7:\n",
    "        continue\n",
    "    \n",
    "    # Получаем последние TIME_STEPS данных для предсказания\n",
    "    last_data = df_ticker.iloc[-TIME_STEPS:]\n",
    "    last_close_prices = last_data['Adj Close'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Масштабируем данные\n",
    "    scaler = scalers[ticker]\n",
    "    scaled_data = scaler.transform(last_close_prices)\n",
    "    \n",
    "    # Формируем входные данные для модели\n",
    "    X_input = np.array([scaled_data])\n",
    "    \n",
    "    # Делаем предсказание на 7 дней вперед\n",
    "    # Рекурсивное предсказание\n",
    "    predictions = []\n",
    "    current_input = X_input.copy()\n",
    "    for _ in range(7):\n",
    "        predicted_price = models[ticker].predict(current_input)\n",
    "        predictions.append(predicted_price[0, 0])\n",
    "        \n",
    "        # Преобразуем predicted_price в правильную форму\n",
    "        predicted_price_reshaped = predicted_price.reshape((1, 1, 1))\n",
    "        \n",
    "        # Обновляем current_input, удаляя первый временной шаг и добавляя предсказанное значение\n",
    "        current_input = np.concatenate((current_input[:, 1:, :], predicted_price_reshaped), axis=1)\n",
    "    \n",
    "    # Обратное масштабирование предсказанных цен\n",
    "    predicted_prices = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    \n",
    "    # Текущая цена\n",
    "    current_price = last_close_prices[-1, 0]\n",
    "    \n",
    "    # Предсказанная цена через 7 дней\n",
    "    predicted_price_7d = predicted_prices[-1, 0]\n",
    "    \n",
    "    # Рассчитываем ожидаемую доходность\n",
    "    expected_return = (predicted_price_7d - current_price) / current_price\n",
    "    \n",
    "    # Сохраняем ожидаемую доходность и волатильность\n",
    "    expected_returns[ticker] = {\n",
    "        'Expected Return': expected_return,\n",
    "        'Volatility': volatility.get(ticker, np.nan),\n",
    "        'Sector': sectors_df[sectors_df['Ticker'] == ticker]['Sector'].values[0] if ticker in sectors_df['Ticker'].values else 'Unknown'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb866-da3d-4afe-a6d8-b6f04a52f574",
   "metadata": {},
   "source": [
    "#### Сортируем акции по коэффициенту Шарпа и отбираем топ-10 с учетом диверсификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e578e77a-e3da-4501-81c2-6b04c42d441c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создаем DataFrame с информацией о тикерах\n",
    "expected_returns_df = pd.DataFrame.from_dict(expected_returns, orient='index')\n",
    "\n",
    "# Удаляем строки с отсутствующими значениями\n",
    "expected_returns_df = expected_returns_df.dropna()\n",
    "\n",
    "# Рассчитываем коэффициент Шарпа\n",
    "expected_returns_df['Sharpe Ratio'] = expected_returns_df['Expected Return'] / expected_returns_df['Volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ec9a9b4-9f08-4161-bf10-d4f067d69e24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранные акции для портфеля:\n",
      "      Expected Return  Volatility  Sharpe Ratio                  Sector\n",
      "NCLH         0.244325    0.019892     12.282724       Consumer Cyclical\n",
      "UAL          0.212057    0.019892     10.660564             Industrials\n",
      "MRO          0.188018    0.019892      9.452060                  Energy\n",
      "BAX          0.179797    0.019892      9.038778              Healthcare\n",
      "SWK          0.176865    0.019892      8.891355             Industrials\n",
      "MTCH         0.146464    0.019892      7.363024  Communication Services\n",
      "PARA         0.146294    0.019892      7.354493  Communication Services\n",
      "KMX          0.139848    0.019892      7.030437       Consumer Cyclical\n",
      "CPAY         0.123951    0.019892      6.231249              Technology\n",
      "PRU          0.114168    0.019892      5.739436      Financial Services\n"
     ]
    }
   ],
   "source": [
    "# Сортируем акции по коэффициенту Шарпа в порядке убывания\n",
    "expected_returns_df = expected_returns_df.sort_values(by='Sharpe Ratio', ascending=False)\n",
    "\n",
    "# Отбираем топ-10 акций с учетом диверсификации по секторам\n",
    "selected_tickers = []\n",
    "sector_counts = {}\n",
    "\n",
    "for idx, row in expected_returns_df.iterrows():\n",
    "    sector = row['Sector']\n",
    "    # Инициализируем счетчик для сектора\n",
    "    if sector not in sector_counts:\n",
    "        sector_counts[sector] = 0\n",
    "    # Проверяем, сколько акций из этого сектора уже выбрано\n",
    "    if sector_counts[sector] < 2:  # Ограничение: не более 2 акций из одного сектора\n",
    "        selected_tickers.append(idx)\n",
    "        sector_counts[sector] += 1\n",
    "    if len(selected_tickers) == 10:\n",
    "        break\n",
    "\n",
    "# Проверяем, что набрано 10 акций\n",
    "if len(selected_tickers) < 10:\n",
    "    # Если не удалось набрать 10 акций с учетом ограничений по секторам, добираем оставшиеся акции\n",
    "    remaining_tickers = expected_returns_df.index.difference(selected_tickers)\n",
    "    for ticker in remaining_tickers:\n",
    "        selected_tickers.append(ticker)\n",
    "        if len(selected_tickers) == 10:\n",
    "            break\n",
    "\n",
    "# Финальный DataFrame с выбранными акциями\n",
    "portfolio_df = expected_returns_df.loc[selected_tickers]\n",
    "\n",
    "print(\"Выбранные акции для портфеля:\")\n",
    "print(portfolio_df[['Expected Return', 'Volatility', 'Sharpe Ratio', 'Sector']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072adca2-7687-4121-9037-70f634763c8a",
   "metadata": {},
   "source": [
    "#### Расчет долей инвестиций по критерию Келли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4cd0253e-ab0f-4b15-bc4d-5b26f8e0f4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение капитала по акциям:\n",
      "      Expected Return  Volatility  Sharpe Ratio                  Sector  \\\n",
      "NCLH         0.244325    0.019892     12.282724       Consumer Cyclical   \n",
      "UAL          0.212057    0.019892     10.660564             Industrials   \n",
      "MRO          0.188018    0.019892      9.452060                  Energy   \n",
      "BAX          0.179797    0.019892      9.038778              Healthcare   \n",
      "SWK          0.176865    0.019892      8.891355             Industrials   \n",
      "MTCH         0.146464    0.019892      7.363024  Communication Services   \n",
      "PARA         0.146294    0.019892      7.354493  Communication Services   \n",
      "KMX          0.139848    0.019892      7.030437       Consumer Cyclical   \n",
      "CPAY         0.123951    0.019892      6.231249              Technology   \n",
      "PRU          0.114168    0.019892      5.739436      Financial Services   \n",
      "\n",
      "      Investment Fraction  \n",
      "NCLH                  0.1  \n",
      "UAL                   0.1  \n",
      "MRO                   0.1  \n",
      "BAX                   0.1  \n",
      "SWK                   0.1  \n",
      "MTCH                  0.1  \n",
      "PARA                  0.1  \n",
      "KMX                   0.1  \n",
      "CPAY                  0.1  \n",
      "PRU                   0.1  \n"
     ]
    }
   ],
   "source": [
    "# Расчет долей инвестиций по критерию Келли\n",
    "portfolio_df['Kelly Fraction'] = portfolio_df.apply(\n",
    "    lambda row: row['Expected Return'] / (row['Volatility'] ** 2), axis=1\n",
    ")\n",
    "\n",
    "# Убираем отрицательные значения и заменяем их на 0\n",
    "portfolio_df['Kelly Fraction'] = portfolio_df['Kelly Fraction'].apply(lambda x: max(x, 0))\n",
    "\n",
    "# Ограничиваем максимальную долю в одной акции (например, 20%)\n",
    "max_fraction = 0.2\n",
    "portfolio_df['Kelly Fraction'] = portfolio_df['Kelly Fraction'].apply(lambda x: min(x, max_fraction))\n",
    "\n",
    "# Проверяем сумму долей\n",
    "total_fraction = portfolio_df['Kelly Fraction'].sum()\n",
    "\n",
    "# Если сумма долей больше 1, нормализуем доли\n",
    "if total_fraction > 1:\n",
    "    portfolio_df['Kelly Fraction'] = portfolio_df['Kelly Fraction'] / total_fraction\n",
    "\n",
    "# Финальные доли инвестиций\n",
    "portfolio_df['Investment Fraction'] = portfolio_df['Kelly Fraction']\n",
    "\n",
    "print(\"Распределение капитала по акциям:\")\n",
    "print(portfolio_df[['Expected Return', 'Volatility', 'Sharpe Ratio', 'Sector', 'Investment Fraction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c394aac-018a-48d6-85b7-500c46cfd249",
   "metadata": {},
   "source": [
    "#### Оценка доходности портфеля через неделю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b85dfd8c-1514-476a-a15d-522723d7216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фактическая доходность портфеля через неделю: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Рассчитываем фактическую доходность каждой акции через неделю\n",
    "actual_returns = {}\n",
    "for ticker in selected_tickers:\n",
    "    df_ticker = data[data['Ticker'] == ticker].sort_values('Date').reset_index(drop=True)\n",
    "    idx_current = df_ticker[df_ticker['Date'] == prediction_date].index\n",
    "    if len(idx_current) == 0:\n",
    "        continue\n",
    "    idx_current = idx_current[0]\n",
    "    if idx_current + 7 >= len(df_ticker):\n",
    "        continue\n",
    "    actual_price_7d = df_ticker.iloc[idx_current + 7]['Adj Close']\n",
    "    current_price = df_ticker.iloc[idx_current]['Adj Close']\n",
    "    actual_return = (actual_price_7d - current_price) / current_price\n",
    "    actual_returns[ticker] = actual_return\n",
    "\n",
    "# Добавляем фактическую доходность в DataFrame портфеля\n",
    "portfolio_df['Actual Return'] = portfolio_df.index.map(actual_returns)\n",
    "\n",
    "# Заполняем отсутствующие значения нулями\n",
    "portfolio_df['Actual Return'] = portfolio_df['Actual Return'].fillna(0)\n",
    "\n",
    "# Рассчитываем общую доходность портфеля\n",
    "portfolio_return = (portfolio_df['Actual Return'] * portfolio_df['Investment Fraction']).sum()\n",
    "\n",
    "print(f\"Фактическая доходность портфеля через неделю: {portfolio_return * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a52e44-3873-45dd-8cd3-45af95b7ba27",
   "metadata": {},
   "source": [
    "# Этап 4: пересмотр портфеля"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372d697-8b9e-46b0-92e0-c113205fa671",
   "metadata": {},
   "source": [
    "#### Загрузка данных по индексу S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "afc53ab5-b112-485d-b20c-2798ddc99abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загрузка данных по индексу\n",
    "sp500_data = pd.read_csv('data/sp500_index_data.csv')\n",
    "\n",
    "# Преобразование столбца 'Date' в формат datetime\n",
    "sp500_data['Date'] = pd.to_datetime(sp500_data['Date'])\n",
    "\n",
    "# Сортировка данных по дате\n",
    "sp500_data = sp500_data.sort_values('Date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b58e80-3da1-48b3-a3d0-96121d835007",
   "metadata": {},
   "source": [
    "#### Расчет ежедневной доходности индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d86b43d3-8c14-4b4b-a91e-32ba5616d31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Расчет ежедневной доходности индекса\n",
    "sp500_data['Return'] = sp500_data['Adj Close'].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be735d66-d549-41b5-8b66-cf798bcad402",
   "metadata": {},
   "source": [
    "#### Сравнение доходности портфелей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2cc07232-1f99-4184-afd5-3eb355408882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Даты пересмотра портфеля:\n",
      "DatetimeIndex(['2022-05-26', '2022-06-03', '2022-06-10', '2022-06-17',\n",
      "               '2022-06-27', '2022-07-05', '2022-07-12', '2022-07-19',\n",
      "               '2022-07-26', '2022-08-02', '2022-08-09', '2022-08-16',\n",
      "               '2022-08-23', '2022-08-30', '2022-09-07', '2022-09-14',\n",
      "               '2022-09-21', '2022-09-28', '2022-10-05', '2022-10-12',\n",
      "               '2022-10-19', '2022-10-26', '2022-11-02', '2022-11-09',\n",
      "               '2022-11-16', '2022-11-23', '2022-12-01', '2022-12-08',\n",
      "               '2022-12-15', '2022-12-22', '2022-12-30'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Шаг 1: Загрузка тестовых данных\n",
    "test_data = pd.read_csv('data/sp500_test_data.csv')\n",
    "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
    "test_data = test_data.sort_values(['Date']).reset_index(drop=True)\n",
    "\n",
    "# Шаг 2: Получение списка уникальных торговых дат\n",
    "test_dates = sorted(test_data['Date'].unique())\n",
    "\n",
    "# Шаг 3: Выбор каждой 5-й даты\n",
    "review_dates = test_dates[::5]\n",
    "\n",
    "# Шаг 4: Преобразование в DatetimeIndex, если необходимо\n",
    "review_dates = pd.to_datetime(review_dates)\n",
    "\n",
    "# Теперь review_dates содержит даты пересмотра портфеля\n",
    "print(\"Даты пересмотра портфеля:\")\n",
    "print(review_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04bf65-dbb7-49c3-9c12-3f71410a2bd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Пересмотр портфеля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "64885576-4531-446b-aaca-88a2405fd2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пересмотр портфеля на дату: 2022-05-26\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-06-03\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-06-10\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-06-17\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-06-27\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-07-05\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-07-12\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-07-19\n",
      "Нет доступных данных для составления портфеля на эту дату.\n",
      "Пересмотр портфеля на дату: 2022-07-26\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Ticker'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 65\u001b[0m\n\u001b[1;32m     59\u001b[0m     expected_return \u001b[38;5;241m=\u001b[39m (predicted_price_7d \u001b[38;5;241m-\u001b[39m current_price) \u001b[38;5;241m/\u001b[39m current_price\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Сохраняем ожидаемую доходность и волатильность\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     expected_returns[ticker] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected Return\u001b[39m\u001b[38;5;124m'\u001b[39m: expected_return,\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolatility\u001b[39m\u001b[38;5;124m'\u001b[39m: volatility\u001b[38;5;241m.\u001b[39mget(ticker, np\u001b[38;5;241m.\u001b[39mnan),\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSector\u001b[39m\u001b[38;5;124m'\u001b[39m: sectors_df[sectors_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ticker][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSector\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m sectors_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m     }\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Преобразуем словарь в DataFrame\u001b[39;00m\n\u001b[1;32m     69\u001b[0m expected_returns_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(expected_returns, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Ticker'"
     ]
    }
   ],
   "source": [
    "portfolio_history = []\n",
    "portfolio_returns = []\n",
    "index_returns = []\n",
    "dates = []\n",
    "\n",
    "for i in range(len(review_dates) - 1):\n",
    "    current_date = review_dates[i]\n",
    "    next_date = review_dates[i + 1]\n",
    "    print(f\"Пересмотр портфеля на дату: {current_date.date()}\")\n",
    "\n",
    "    # Словарь для хранения ожидаемой доходности и волатильности\n",
    "    expected_returns = {}\n",
    "\n",
    "    for ticker in clean_tickers:\n",
    "        if ticker not in models:\n",
    "            continue\n",
    "        # Получаем данные для тикера до текущей даты\n",
    "        df_ticker = test_data[(test_data['Ticker'] == ticker) & (test_data['Date'] <= current_date)].sort_values('Date')\n",
    "\n",
    "        # Проверяем, есть ли достаточное количество данных\n",
    "        if len(df_ticker) < TIME_STEPS + 7:\n",
    "            continue\n",
    "\n",
    "        # Получаем последние TIME_STEPS данных для предсказания\n",
    "        last_data = df_ticker.iloc[-TIME_STEPS:]\n",
    "        last_close_prices = last_data['Adj Close'].values.reshape(-1, 1)\n",
    "\n",
    "        # Масштабируем данные\n",
    "        scaler = scalers[ticker]\n",
    "        scaled_data = scaler.transform(last_close_prices)\n",
    "\n",
    "        # Формируем входные данные для модели\n",
    "        X_input = np.array([scaled_data])\n",
    "\n",
    "        # Делаем предсказание на 7 дней вперед\n",
    "        # Рекурсивное предсказание\n",
    "        predictions = []\n",
    "        current_input = X_input.copy()\n",
    "        for _ in range(7):\n",
    "            predicted_price = models[ticker].predict(current_input)\n",
    "            predictions.append(predicted_price[0, 0])\n",
    "\n",
    "            # Преобразуем predicted_price в правильную форму\n",
    "            predicted_price_reshaped = predicted_price.reshape((1, 1, 1))\n",
    "\n",
    "            # Обновляем current_input\n",
    "            current_input = np.concatenate((current_input[:, 1:, :], predicted_price_reshaped), axis=1)\n",
    "\n",
    "        # Обратное масштабирование предсказанных цен\n",
    "        predicted_prices = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "        # Текущая цена\n",
    "        current_price = last_close_prices[-1, 0]\n",
    "\n",
    "        # Предсказанная цена через 7 дней\n",
    "        predicted_price_7d = predicted_prices[-1, 0]\n",
    "\n",
    "        # Рассчитываем ожидаемую доходность\n",
    "        expected_return = (predicted_price_7d - current_price) / current_price\n",
    "\n",
    "        # Сохраняем ожидаемую доходность и волатильность\n",
    "        expected_returns[ticker] = {\n",
    "            'Expected Return': expected_return,\n",
    "            'Volatility': volatility.get(ticker, np.nan),\n",
    "            'Sector': sectors_df[sectors_df['Ticker'] == ticker]['Sector'].values[0] if ticker in sectors_df['Ticker'].values else 'Unknown'\n",
    "        }\n",
    "\n",
    "    # Преобразуем словарь в DataFrame\n",
    "    expected_returns_df = pd.DataFrame.from_dict(expected_returns, orient='index').dropna()\n",
    "\n",
    "    # Продолжаем только если есть данные\n",
    "    if expected_returns_df.empty:\n",
    "        print(\"Нет доступных данных для составления портфеля на эту дату.\")\n",
    "        continue\n",
    "\n",
    "    # Остальная часть кода остается без изменений...\n",
    "    # Отбор акций, расчет долей по критерию Келли, оценка доходности и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd5bca-ebc7-4de0-8025-3b8c6e829106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
